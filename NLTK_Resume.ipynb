{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kF_HO_XX76xIuzJ2FOdansxuK9biqjts",
      "authorship_tag": "ABX9TyNTASt7lYXnJMPS6gL4TK9D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpit-parejiya01/Project/blob/main/NLTK_Resume.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI419X8GVmiZ",
        "outputId": "dda9ea1c-d930-43e3-ee8e-6e717184c9ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!pip install opencv-python\n",
        "!pip install pillow\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-34zTzHa4Z6",
        "outputId": "072eaa13-dd7a-4e77-bd4d-8dd8b27c6630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (4,698 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123599 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.4.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New Final"
      ],
      "metadata": {
        "id": "ZLpqCgrOywxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "from pytesseract import Output\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources (if not already done)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Stronger extraction by adding specific keywords and patterns\n",
        "def extract_resume_details(text):\n",
        "    # Initialize extracted information\n",
        "    details = {\n",
        "        \"name\": None,\n",
        "        \"mobile_number\": None,\n",
        "        \"email\": None,\n",
        "        \"linkedin\": None,\n",
        "        \"education\": None,\n",
        "        \"skills\": None,\n",
        "        \"experience\": None,\n",
        "        \"projects\": None,\n",
        "        \"Achivements\" : None\n",
        "    }\n",
        "\n",
        "    # Regular expressions for extracting specific information\n",
        "    mobile_pattern = re.compile(r'\\b(\\+?\\d{1,4}[-.\\s]?)?(\\d{10}|\\d{3}[-.\\s]\\d{3}[-.\\s]\\d{4})\\b')\n",
        "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')\n",
        "    linkedin_pattern = re.compile(r'\\b(https?://)?(www\\.)?linkedin\\.com/in/[A-Za-z0-9_-]+/?\\b')\n",
        "\n",
        "    # Define strong keywords or patterns for different sections\n",
        "    patterns = {\n",
        "        \"education\": r\"(education|bachelor|master|university|school|college|degree)\",\n",
        "        \"skills\": r\"(skills|proficiencies|technologies|competencies|technical skills)\",\n",
        "        \"experience\": r\"(experience|employment|work history|internship|professional experience)\",\n",
        "        \"projects\": r\"(projects|portfolio|case study|assignments)\",\n",
        "        \"Achivements\": r\"(certificate|awards|achivements)\"\n",
        "    }\n",
        "\n",
        "    # Split text into lines\n",
        "    lines = text.split(\"\\n\")\n",
        "    current_section = None\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Extract mobile number\n",
        "        if not details[\"mobile_number\"]:\n",
        "            mobile_match = re.search(mobile_pattern, line)\n",
        "            if mobile_match:\n",
        "                details[\"mobile_number\"] = mobile_match.group()\n",
        "\n",
        "        # Extract email address\n",
        "        if not details[\"email\"]:\n",
        "            email_match = re.search(email_pattern, line)\n",
        "            if email_match:\n",
        "                details[\"email\"] = email_match.group()\n",
        "\n",
        "        # Extract LinkedIn link\n",
        "        if not details[\"linkedin\"]:\n",
        "            linkedin_match = re.search(linkedin_pattern, line)\n",
        "            if linkedin_match:\n",
        "                details[\"linkedin\"] = linkedin_match.group()\n",
        "\n",
        "        # Extract name (basic heuristic: first line, proper case)\n",
        "        if not details[\"name\"] and re.match(r'^[A-Z][a-z]+\\s[A-Z][a-z]+', line):\n",
        "            details[\"name\"] = line\n",
        "\n",
        "        # Identify sections using strong keywords\n",
        "        for section, pattern in patterns.items():\n",
        "            if re.search(pattern, line.lower()):\n",
        "                current_section = section\n",
        "                if details[current_section] is None:\n",
        "                    details[current_section] = []\n",
        "                break\n",
        "\n",
        "        # Append lines to the current section\n",
        "        if current_section and details[current_section] is not None:\n",
        "            details[current_section].append(line)\n",
        "\n",
        "    return details"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7gzotKfywQO",
        "outputId": "9f7d46f8-cf55-45c5-ef23-3595e3492a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pattern-change\n",
        "\n"
      ],
      "metadata": {
        "id": "XatI3705M7sR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------"
      ],
      "metadata": {
        "id": "bazSXslhM4vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "from pytesseract import Output\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources (if not already done)\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# Stronger extraction by adding specific keywords and patterns\n",
        "def extract_resume_details(text):\n",
        "    # Initialize extracted information\n",
        "    details = {\n",
        "        \"name\": None,\n",
        "        \"mobile_number\": None,\n",
        "        \"email\": None,\n",
        "        \"linkedin\": None,\n",
        "        \"education\": None,\n",
        "        \"skills\": None,\n",
        "        \"experience\": None,\n",
        "        \"projects\": None,\n",
        "        \"achivements\": None,\n",
        "        \"career-objective\": None\n",
        "    }\n",
        "\n",
        "    # Regular expressions for extracting specific information\n",
        "    mobile_pattern = re.compile(r'\\+?\\d{1,3}[-.\\s]?\\(?\\d{2,4}\\)?[-.\\s]?\\d{3,4}[-.\\s]?\\d{4,6}\\b')\n",
        "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
        "    linkedin_pattern = re.compile(r'\\b(?:https?:\\/\\/)?(?:www\\.)?linkedin\\.com\\/(?:in\\/[\\w-]+|pub\\/[\\w-]+|company\\/[\\w-]+|school\\/[\\w-]+|groups\\/[\\w-]+|events\\/[\\w-]+|[\\w-]+(?:\\/[\\w-]*)?)\\b')\n",
        "\n",
        "    # Define strong keywords or patterns for different sections\n",
        "    patterns = {\n",
        "        \"education\": r\"(education|bachelor|master's|master)\",\n",
        "        \"skills\": r\"(skills|technical skills)\",\n",
        "        \"experience\": r\"(work experience|professional experience|experience)\",\n",
        "        \"career-objective\": r\"(career objective|objective|professional summary|career summary|summary)\",\n",
        "        \"projects\": r\"(projects|project)\",\n",
        "        \"achivements\": r\"(certificate|awards|achivements|certifications)\"\n",
        "    }\n",
        "\n",
        "    # Split text into lines\n",
        "    lines = text.split(\"\\n\")\n",
        "    current_section = None\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Extract mobile number\n",
        "        if not details[\"mobile_number\"]:\n",
        "            mobile_match = re.search(mobile_pattern, line)\n",
        "            if mobile_match:\n",
        "                details[\"mobile_number\"] = mobile_match.group()\n",
        "\n",
        "\n",
        "        # Extract email address\n",
        "        if not details[\"email\"]:\n",
        "            email_match = re.search(email_pattern, line)\n",
        "            if email_match:\n",
        "                details[\"email\"] = email_match.group()\n",
        "\n",
        "        # Extract LinkedIn link\n",
        "        if not details[\"linkedin\"]:\n",
        "            linkedin_match = re.search(linkedin_pattern, line)\n",
        "            if linkedin_match:\n",
        "                details[\"linkedin\"] = linkedin_match.group()\n",
        "\n",
        "        # Extract name (basic heuristic: first line, proper case)\n",
        "        if not details[\"name\"] and re.match(r'(?<![\\w\\d])([A-Z][a-zA-Z]{1,}(?:\\s[A-Z][a-zA-Z]{1,}){0,2})(?![\\w\\d])', line):\n",
        "            details[\"name\"] = line\n",
        "\n",
        "        # Identify sections using strong keywords\n",
        "        for section, pattern in patterns.items():\n",
        "            if re.search(pattern, line.lower()):\n",
        "                current_section = section\n",
        "                if details[current_section] is None:\n",
        "                    details[current_section] = []\n",
        "                break\n",
        "\n",
        "        # Append lines to the current section\n",
        "        if current_section and details[current_section] is not None:\n",
        "            details[current_section].append(line)\n",
        "\n",
        "    return details"
      ],
      "metadata": {
        "id": "hc6uFnxCM3cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "# Load image and perform OCR with Tesseract\n",
        "\n",
        "image = Image.open(\"/content/University.jpg\")  # Load the uploaded image\n",
        "\n",
        "extracted_text = pytesseract.image_to_string(image)\n",
        "sections = extract_resume_details(extracted_text)\n",
        "\n",
        "# Print the extracted sections\n",
        "for section, content in sections.items():\n",
        "    print(f\"{section.upper()}:\\n{' '.join(content) if content else 'Not Found'}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGqRuZBJy7Rk",
        "outputId": "64572486-0175-4ada-b669-8aa3f1d073e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME:\n",
            "M E R L L A N G W O R T H\n",
            "\n",
            "MOBILE_NUMBER:\n",
            "Not Found\n",
            "\n",
            "EMAIL:\n",
            "M e r l m a i l @ m a i l . c o m\n",
            "\n",
            "LINKEDIN:\n",
            "Not Found\n",
            "\n",
            "EDUCATION:\n",
            "EDUCATION  MEGGIETON HIGH SCHOOL, MEGGIETON, KANSAS 2022 - Present | GPA: 3.75 \n",
            "\n",
            "SKILLS:\n",
            "TECHNICAL SKILLS  e Computer e Math e Microsoft Office  ACTIVITIES  VELIT BIBENDUM HIGH SCHOOL TENNIS TEAM 2021 - Present  KEY SKILLS  e Friendly  e Good listener  e Courteous  e Helpful  e Guest services e Interpersonal  e Positive attitude  e Customer service \n",
            "\n",
            "EXPERIENCE:\n",
            "EXPERIENCE  PET SITTER 2021 - Present  e Suspendisse eu sollicitudin tellus. Aliquam quam nisi, hen- drerit ut ex id, pretium elementum augue.  e Donec leo ligula, eleifend id molestie nec, porttitor id metus. Nunc lacinia viverra dolor, id condimentum arcu tristique.  CHILD CARE 2021 - Present  e Morbi placerat odio vitae varius ornare. Donec non tempor lacus. Sed sit amet neque id ligula suscipit rhoncus.  e Maecenas vestibulum, elit non interdum tempor, felis orci efficitur ante, ut laoreet eros diam at libero.  VOLUNTEER WORK SUSPENDISSE UT JUSTO COACH 2021 - Present  IN PHARETRA ARCU PROGRAM 2022 - Present \n",
            "\n",
            "PROJECTS:\n",
            "Not Found\n",
            "\n",
            "ACHIVEMENTS:\n",
            "Not Found\n",
            "\n",
            "CAREER-OBJECTIVE:\n",
            "SUMMARY  Donec luctus id eros ac tristique. Nulla volutpat est eget eleif- end blandit. Nulla sagittis, sapien quis mollis ullamcorper, lorem ligula fringilla lorem, id congue justo mi at urna. Ali- quam eget metus lectus quisque euismod dui vel elit imperdi- et lobortis. Maecenas auctor orci lacus. Integer vulputate turpis sit amet ex vehicula ornare. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "\n",
        "# Load the image\n",
        "image_path = '/content/drive/MyDrive/hindi_books_RnD/data/images/018_0.png'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Pre-process the image for better OCR results\n",
        "# Convert to grayscale\n",
        "image = image.convert('L')\n",
        "\n",
        "# Enhance the contrast\n",
        "enhancer = ImageEnhance.Contrast(image)\n",
        "image = enhancer.enhance(2)\n",
        "\n",
        "# Apply some sharpening\n",
        "image = image.filter(ImageFilter.SHARPEN)\n",
        "\n",
        "# Resize the image if necessary to improve OCR accuracy\n",
        "width, height = image.size\n",
        "image = image.resize((width * 3, height * 3))\n",
        "\n",
        "# Perform OCR on the processed image\n",
        "text = pytesseract.image_to_string(image)\n",
        "\n",
        "# print(text)\n",
        "\n",
        "sections = extract_resume_details(text)\n",
        "\n",
        "# Print the extracted sections\n",
        "for section, content in sections.items():\n",
        "    print(f\"{section.upper()}:\\n{' '.join(content) if content else 'Not Found'}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqKnnwwrsf0U",
        "outputId": "d1404399-ab86-47b6-91d9-85ff113073df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME:\n",
            "X a n d e r   C l e m m o n s\n",
            "\n",
            "MOBILE_NUMBER:\n",
            "Not Found\n",
            "\n",
            "EMAIL:\n",
            "x c l e m m @ e m a i l . c o m\n",
            "\n",
            "LINKEDIN:\n",
            "Not Found\n",
            "\n",
            "EDUCATION:\n",
            "Florida Polytechnic University - Web Designer April 2016 - January 2020 Lakeland, FL - Created and assisted with the development of 3 official university-sponsored websites.  - Created and assisted with the design, develooment, and support of new and existing secure websites and web applications.  - Conceptualized, created, and managed dynamic web pages for data display and entry using EDUCATION  University of Florida - B.S., Computer Science September 2011 - June 2015 Gainesville, FL \n",
            "\n",
            "SKILLS:\n",
            "appropriate technologies (HTML, PHP, and Web CMS).  - Worked as part of a 4-person team, and communicated In a professional and collegial way. - Provided training for continuous updating of websites.  Hear.com - Front-End Developer Intern June 2015 - April 2016 Miami, FL - Translated designs and wireframes into high-quality code using HTML, CSS, and JavaScript.  - Worked closely with developers, designers, copywriters, and other cross-functional teams (CRO, Video, Marketing, etc..) to drive innovation and maximize conversions by 40%.  - Provided input to leaders 3 times a year about the future develooment of new features. - Ensured the technical feasibility of UI/UX designs.  SKILLS  Python; JavaScript; HTML; CSS; Java; PHP React.Jjs; jQuery; Selenium  Angular.js; Laravel  MongoDB; Jest; Enymze; Mocha/Chai \n",
            "\n",
            "EXPERIENCE:\n",
            "WORK EXPERIENCE Squarespace - Senior Web Developer January 2020 - current Remote  - Led and managed a team of 12 developers and designers to create efficient, effective, and visually aesthetic websites for 60,000+ clients.  - Oversaw concept mock-up and wireframe design to further client satisfaction by 35%. - Developed customer-specific design framework, reducing site production time by 4 days.  - Collaborated with VP of sales to create digital marketing designs that led to an average 20,000+ additional clients per month.  - Led bi-weekly standup to celebrate team successes, address weaknesses, and assign tasks. \n",
            "\n",
            "PROJECTS:\n",
            "Not Found\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "MODY4NnsC8i1",
        "outputId": "ce97cc73-faf1-4186-bd91-b78d25b773b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Xander Clemmons\\n\\n \\n\\nxclemm@email.com (123) 456-7890 Tampa, FL xander-codes.dev\\nLinkedIn Github\\nWORK EXPERIENCE\\nSquarespace - Senior Web Developer\\nJanuary 2020 - current Remote\\n\\n- Led and managed a team of 12 developers and designers to create efficient, effective, and\\nvisually aesthetic websites for 60,000+ clients.\\n\\n- Oversaw concept mock-up and wireframe design to further client satisfaction by 35%.\\n- Developed customer-specific design framework, reducing site production time by 4 days.\\n\\n- Collaborated with VP of sales to create digital marketing designs that led to an average\\n20,000+ additional clients per month.\\n\\n- Led bi-weekly standup to celebrate team successes, address weaknesses, and assign tasks.\\n\\nFlorida Polytechnic University - Web Designer\\nApril 2016 - January 2020 Lakeland, FL\\n- Created and assisted with the development of 3 official university-sponsored websites.\\n\\n- Created and assisted with the design, develooment, and support of new and existing secure\\nwebsites and web applications.\\n\\n- Conceptualized, created, and managed dynamic web pages for data display and entry using\\nappropriate technologies (HTML, PHP, and Web CMS).\\n\\n- Worked as part of a 4-person team, and communicated In a professional and collegial way.\\n- Provided training for continuous updating of websites.\\n\\nHear.com - Front-End Developer Intern\\nJune 2015 - April 2016 Miami, FL\\n- Translated designs and wireframes into high-quality code using HTML, CSS, and JavaScript.\\n\\n- Worked closely with developers, designers, copywriters, and other cross-functional teams\\n(CRO, Video, Marketing, etc..) to drive innovation and maximize conversions by 40%.\\n\\n- Provided input to leaders 3 times a year about the future develooment of new features.\\n- Ensured the technical feasibility of UI/UX designs.\\n\\nEDUCATION\\n\\nUniversity of Florida - B.S., Computer Science\\nSeptember 2011 - June 2015 Gainesville, FL\\n\\nSKILLS\\n\\nPython; JavaScript; HTML; CSS; Java; PHP\\nReact.Jjs; jQuery; Selenium\\n\\nAngular.js; Laravel\\n\\nMongoDB; Jest; Enymze; Mocha/Chai\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Codes"
      ],
      "metadata": {
        "id": "00IsAnz-j7Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "from pytesseract import Output\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download NLTK resources (if not already done)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Stronger extraction by adding specific keywords and patterns\n",
        "def extract_resume_details(text):\n",
        "    # Initialize extracted information\n",
        "    details = {\n",
        "        \"name\": None,\n",
        "        \"mobile_number\": None,\n",
        "        \"email\": None,\n",
        "        \"linkedin\": None,\n",
        "        \"education\": None,\n",
        "        \"skills\": None,\n",
        "        \"experience\": None,\n",
        "        \"projects\": None,\n",
        "        \"designation\": None\n",
        "    }\n",
        "\n",
        "    # Regular expressions for extracting specific information\n",
        "    mobile_pattern = re.compile(r'\\b(\\+?\\d{1,4}[-.\\s]?)?(\\d{10}|\\d{3}[-.\\s]\\d{3}[-.\\s]\\d{4})\\b')\n",
        "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')\n",
        "    linkedin_pattern = re.compile(r'\\b(https?://)?(www\\.)?linkedin\\.com/in/[A-Za-z0-9_-]+/?\\b')\n",
        "\n",
        "    # Common job titles for designation extraction\n",
        "    job_titles = re.compile(r\"(software engineer|data scientist|product manager|business analyst|consultant|\"\n",
        "                            r\"developer|designer|project manager|intern|senior|junior|lead)\", re.I)\n",
        "\n",
        "    # Define strong keywords or patterns for different sections\n",
        "    patterns = {\n",
        "        \"education\": r\"(education|bachelor|master|university|school|college|degree)\",\n",
        "        \"skills\": r\"(skills|proficiencies|technologies|competencies|technical skills)\",\n",
        "        \"experience\": r\"(experience|employment|work history|internship|professional experience)\",\n",
        "        \"projects\": r\"(projects|portfolio|case study|assignments)\"\n",
        "    }\n",
        "\n",
        "    # Split text into lines\n",
        "    lines = text.split(\"\\n\")\n",
        "    current_section = None\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Extract mobile number\n",
        "        if not details[\"mobile_number\"]:\n",
        "            mobile_match = re.search(mobile_pattern, line)\n",
        "            if mobile_match:\n",
        "                details[\"mobile_number\"] = mobile_match.group()\n",
        "\n",
        "        # Extract email address\n",
        "        if not details[\"email\"]:\n",
        "            email_match = re.search(email_pattern, line)\n",
        "            if email_match:\n",
        "                details[\"email\"] = email_match.group()\n",
        "\n",
        "        # Extract LinkedIn link\n",
        "        if not details[\"linkedin\"]:\n",
        "            linkedin_match = re.search(linkedin_pattern, line)\n",
        "            if linkedin_match:\n",
        "                details[\"linkedin\"] = linkedin_match.group()\n",
        "\n",
        "        # Extract name (basic heuristic: first line, proper case)\n",
        "        if not details[\"name\"] and re.match(r'^[A-Z][a-z]+\\s[A-Z][a-z]+', line):\n",
        "            details[\"name\"] = line\n",
        "\n",
        "        # Identify sections using strong keywords\n",
        "        for section, pattern in patterns.items():\n",
        "            if re.search(pattern, line.lower()):\n",
        "                current_section = section\n",
        "                if details[current_section] is None:\n",
        "                    details[current_section] = []\n",
        "                break\n",
        "\n",
        "        # Append lines to the current section\n",
        "        if current_section and details[current_section] is not None:\n",
        "            details[current_section].append(line)\n",
        "\n",
        "        # Extract job titles (designation) in the experience section\n",
        "        if current_section == \"experience\" and not details[\"designation\"]:\n",
        "            job_match = re.search(job_titles, line)\n",
        "            if job_match:\n",
        "                details[\"designation\"] = job_match.group()\n",
        "\n",
        "    # Post-processing: Join section lines into full paragraphs\n",
        "    for section in [\"education\", \"skills\", \"experience\", \"projects\"]:\n",
        "        if details[section]:\n",
        "            details[section] = \" \".join(details[section])\n",
        "\n",
        "    return details\n"
      ],
      "metadata": {
        "id": "nchl18byP00H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f15b6f14-e5f7-49eb-a766-1849332d01f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "# Load image and perform OCR with Tesseract\n",
        "image = Image.open(\"/content/drive/MyDrive/hindi_books_RnD/data/images/020_0.png\")  # Load the uploaded image\n",
        "\n",
        "extracted_text = pytesseract.image_to_string(image)\n",
        "sections = extract_resume_details(extracted_text)\n",
        "print(extracted_text)\n",
        "# Print the extracted sections\n",
        "for section, content in sections.items():\n",
        "    print(f\"{section.upper()}:\\n{' '.join(content) if content else 'Not Found'}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnMRV5-PkC3g",
        "outputId": "7e9cd9c9-4f0a-4b5c-95ea-eaca6032652c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LEOCADIA SNOW\n",
            "\n",
            "WEB DEVELOPER INTERN\n",
            "\n",
            " \n",
            "\n",
            "CONTACT CAREER OBJECTIVE\n",
            "\n",
            "|.snow@email.com As asenior computer science student, | aim to contribute to Docugami’s\n",
            "(123) 456-7890 innovative approach to document engineering as a web development intern.\n",
            "Seattle. WA Eager to sharpen my skills in database programs and markups within a team that\n",
            "Se pushes the boundaries of traditional document management and analysis.\n",
            "LinkedIn\n",
            "WORK EXPERIENCE\n",
            "EDUCATION\n",
            ". Fast Food Worker\n",
            "Bachelor of Science\n",
            "Subway\n",
            "\n",
            "Computer Science\n",
            "\n",
            "University of Washington\n",
            "2021 - current\n",
            "\n",
            "2022 -current / Seattle, WA\n",
            "\n",
            "e Managed 135+ transactions every day at a busy Subway outlet with\n",
            "utmost cash handling accuracy.\n",
            "\n",
            "Seale WA e Integrated MySQL with the POS system to update prices for 18 specials.\n",
            "e Executed version-controlled recipe adjustments and limited-time offer\n",
            "preparations using Git, achieving a 9% uplift in LTO sales.\n",
            "SKILLS . . .\n",
            "e Leda project to redesign the layout of the store for improved workflow,\n",
            "HTML5 shortening the time to complete orders by 6 minutes during rush hours.\n",
            "CSS3\n",
            "JavaScript\n",
            "MongoDB PROJECTS\n",
            "oe ByteBoost Blog\n",
            "i\n",
            "\n",
            "Content Creator\n",
            "2023 - current\n",
            "e Published a\"CSS3 Tips and Tricks\" weekly post that consistently ranked as\n",
            "the most popular weekly content, boosting overall site traffic by 33%.\n",
            "e Introduced tech challenges that encouraged the practical application of\n",
            "blog concepts, leading to a 19% growth in active user comments.\n",
            "e Produced a series on “JavaScript for Beginners” that attracted 283,210+\n",
            "total views, becoming one of ByteBoost Blog’s top 7 most-read series.\n",
            "e Adjusted the content calendar based on reader demand to ensure all\n",
            "content was highly relevant, raising return visits by 11%.\n",
            "\n",
            "BookBuddy Library\n",
            "\n",
            "Participant\n",
            "2021\n",
            "e Collaborated with local schools to promote literacy through BookBuddy\n",
            "Library, fostering partnerships with 4 schools.\n",
            "e Implemented web-based tools using MongoDB to streamline inventory\n",
            "management, shrinking manual cataloging time by 31 minutes.\n",
            "e Organized a community book donation drive that collected 6,188 books,\n",
            "increasing library inventory and expanding available resources by 21%.\n",
            "e Designed a user-friendly eBook lending interface with HTML5, which\n",
            "expanded eBook checkouts by 52%.\n",
            "\f\n",
            "NAME:\n",
            "C o m p u t e r   S c i e n c e\n",
            "\n",
            "MOBILE_NUMBER:\n",
            "Not Found\n",
            "\n",
            "EMAIL:\n",
            "s n o w @ e m a i l . c o m\n",
            "\n",
            "LINKEDIN:\n",
            "Not Found\n",
            "\n",
            "EDUCATION:\n",
            "E D U C A T I O N   .   F a s t   F o o d   W o r k e r   B a c h e l o r   o f   S c i e n c e   S u b w a y     C o m p u t e r   S c i e n c e     U n i v e r s i t y   o f   W a s h i n g t o n   2 0 2 1   -   c u r r e n t     2 0 2 2   - c u r r e n t   /   S e a t t l e ,   W A     e   M a n a g e d   1 3 5 +   t r a n s a c t i o n s   e v e r y   d a y   a t   a   b u s y   S u b w a y   o u t l e t   w i t h   u t m o s t   c a s h   h a n d l i n g   a c c u r a c y .     S e a l e   W A   e   I n t e g r a t e d   M y S Q L   w i t h   t h e   P O S   s y s t e m   t o   u p d a t e   p r i c e s   f o r   1 8   s p e c i a l s .   e   E x e c u t e d   v e r s i o n - c o n t r o l l e d   r e c i p e   a d j u s t m e n t s   a n d   l i m i t e d - t i m e   o f f e r   p r e p a r a t i o n s   u s i n g   G i t ,   a c h i e v i n g   a   9 %   u p l i f t   i n   L T O   s a l e s .   e   C o l l a b o r a t e d   w i t h   l o c a l   s c h o o l s   t o   p r o m o t e   l i t e r a c y   t h r o u g h   B o o k B u d d y   L i b r a r y ,   f o s t e r i n g   p a r t n e r s h i p s   w i t h   4   s c h o o l s .   e   I m p l e m e n t e d   w e b - b a s e d   t o o l s   u s i n g   M o n g o D B   t o   s t r e a m l i n e   i n v e n t o r y   m a n a g e m e n t ,   s h r i n k i n g   m a n u a l   c a t a l o g i n g   t i m e   b y   3 1   m i n u t e s .   e   O r g a n i z e d   a   c o m m u n i t y   b o o k   d o n a t i o n   d r i v e   t h a t   c o l l e c t e d   6 , 1 8 8   b o o k s ,   i n c r e a s i n g   l i b r a r y   i n v e n t o r y   a n d   e x p a n d i n g   a v a i l a b l e   r e s o u r c e s   b y   2 1 % .   e   D e s i g n e d   a   u s e r - f r i e n d l y   e B o o k   l e n d i n g   i n t e r f a c e   w i t h   H T M L 5 ,   w h i c h   e x p a n d e d   e B o o k   c h e c k o u t s   b y   5 2 % .  \n",
            "\n",
            "SKILLS:\n",
            "S e a t t l e .   W A   E a g e r   t o   s h a r p e n   m y   s k i l l s   i n   d a t a b a s e   p r o g r a m s   a n d   m a r k u p s   w i t h i n   a   t e a m   t h a t   S e   p u s h e s   t h e   b o u n d a r i e s   o f   t r a d i t i o n a l   d o c u m e n t   m a n a g e m e n t   a n d   a n a l y s i s .   L i n k e d I n   S K I L L S   .   .   .   e   L e d a   p r o j e c t   t o   r e d e s i g n   t h e   l a y o u t   o f   t h e   s t o r e   f o r   i m p r o v e d   w o r k f l o w ,   H T M L 5   s h o r t e n i n g   t h e   t i m e   t o   c o m p l e t e   o r d e r s   b y   6   m i n u t e s   d u r i n g   r u s h   h o u r s .   C S S 3   J a v a S c r i p t\n",
            "\n",
            "EXPERIENCE:\n",
            "W O R K   E X P E R I E N C E\n",
            "\n",
            "PROJECTS:\n",
            "M o n g o D B   P R O J E C T S   o e   B y t e B o o s t   B l o g   i     C o n t e n t   C r e a t o r   2 0 2 3   -   c u r r e n t   e   P u b l i s h e d   a \" C S S 3   T i p s   a n d   T r i c k s \"   w e e k l y   p o s t   t h a t   c o n s i s t e n t l y   r a n k e d   a s   t h e   m o s t   p o p u l a r   w e e k l y   c o n t e n t ,   b o o s t i n g   o v e r a l l   s i t e   t r a f f i c   b y   3 3 % .   e   I n t r o d u c e d   t e c h   c h a l l e n g e s   t h a t   e n c o u r a g e d   t h e   p r a c t i c a l   a p p l i c a t i o n   o f   b l o g   c o n c e p t s ,   l e a d i n g   t o   a   1 9 %   g r o w t h   i n   a c t i v e   u s e r   c o m m e n t s .   e   P r o d u c e d   a   s e r i e s   o n   “ J a v a S c r i p t   f o r   B e g i n n e r s ”   t h a t   a t t r a c t e d   2 8 3 , 2 1 0 +   t o t a l   v i e w s ,   b e c o m i n g   o n e   o f   B y t e B o o s t   B l o g ’ s   t o p   7   m o s t - r e a d   s e r i e s .   e   A d j u s t e d   t h e   c o n t e n t   c a l e n d a r   b a s e d   o n   r e a d e r   d e m a n d   t o   e n s u r e   a l l   c o n t e n t   w a s   h i g h l y   r e l e v a n t ,   r a i s i n g   r e t u r n   v i s i t s   b y   1 1 % .     B o o k B u d d y   L i b r a r y     P a r t i c i p a n t   2 0 2 1\n",
            "\n",
            "DESIGNATION:\n",
            "Not Found\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "# Load the SpaCy model for NER\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Designation patterns to help extract job titles\n",
        "designation_keywords = [\n",
        "    \"manager\", \"developer\", \"engineer\", \"designer\", \"analyst\", \"consultant\", \"director\", \"officer\",\n",
        "    \"executive\", \"lead\", \"intern\", \"administrator\", \"architect\", \"specialist\"\n",
        "]\n",
        "\n",
        "def extract_resume_details(text):\n",
        "    # Initialize extracted information\n",
        "    details = {\n",
        "        \"name\": None,\n",
        "        \"mobile_number\": None,\n",
        "        \"email\": None,\n",
        "        \"linkedin\": None,\n",
        "        \"designation\": None,\n",
        "        \"education\": None,\n",
        "        \"skills\": None,\n",
        "        \"experience\": None,\n",
        "        \"projects\": None\n",
        "    }\n",
        "\n",
        "    # Regular expressions for extracting specific information\n",
        "    mobile_pattern = re.compile(r'\\b(\\+?\\d{1,4}[-.\\s]?)?(\\d{10}|\\d{3}[-.\\s]\\d{3}[-.\\s]\\d{4})\\b')\n",
        "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')\n",
        "    linkedin_pattern = re.compile(r'\\b(https?://)?(www\\.)?linkedin\\.com/in/[A-Za-z0-9_-]+/?\\b')\n",
        "\n",
        "    # Define strong keywords or patterns for different sections\n",
        "    section_patterns = {\n",
        "        \"education\": r\"(education|bachelor|master|university|school|college|degree)\",\n",
        "        \"skills\": r\"(skills|proficiencies|technologies|competencies|technical skills)\",\n",
        "        \"experience\": r\"(experience|employment|work history|internship|professional experience)\",\n",
        "        \"projects\": r\"(projects|portfolio|case study|assignments)\"\n",
        "    }\n",
        "\n",
        "    # Split text into lines\n",
        "    lines = text.split(\"\\n\")\n",
        "    current_section = None\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Extract mobile number\n",
        "        if not details[\"mobile_number\"]:\n",
        "            mobile_match = re.search(mobile_pattern, line)\n",
        "            if mobile_match:\n",
        "                details[\"mobile_number\"] = mobile_match.group()\n",
        "\n",
        "        # Extract email address\n",
        "        if not details[\"email\"]:\n",
        "            email_match = re.search(email_pattern, line)\n",
        "            if email_match:\n",
        "                details[\"email\"] = email_match.group()\n",
        "\n",
        "        # Extract LinkedIn link\n",
        "        if not details[\"linkedin\"]:\n",
        "            linkedin_match = re.search(linkedin_pattern, line)\n",
        "            if linkedin_match:\n",
        "                details[\"linkedin\"] = linkedin_match.group()\n",
        "\n",
        "        # Extract name (basic heuristic: SpaCy to identify names)\n",
        "        doc = nlp(line)\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == 'PERSON' and not details[\"name\"]:\n",
        "                details[\"name\"] = ent.text\n",
        "\n",
        "        # Extract designation using keywords and SpaCy NER\n",
        "        for designation in designation_keywords:\n",
        "            if designation.lower() in line.lower() and not details[\"designation\"]:\n",
        "                details[\"designation\"] = line\n",
        "\n",
        "        # Identify sections using strong keywords\n",
        "        for section, pattern in section_patterns.items():\n",
        "            if re.search(pattern, line.lower()):\n",
        "                current_section = section\n",
        "                if details[current_section] is None:\n",
        "                    details[current_section] = []\n",
        "                break\n",
        "\n",
        "        # Append lines to the current section\n",
        "        if current_section and details[current_section] is not None:\n",
        "            details[current_section].append(line)\n",
        "\n",
        "    return details\n",
        "\n",
        "\n",
        "def extract_text_from_image(image_path):\n",
        "    # Load the image using OpenCV\n",
        "    img = cv2.imread(image_path)\n",
        "    # Use Tesseract to extract text\n",
        "    text = pytesseract.image_to_string(img)\n",
        "    return text\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example: Extract text from image and then parse it (if resumes are images)\n",
        "    resume_text = extract_text_from_image(\"/content/drive/MyDrive/hindi_books_RnD/data/images/020_0.png\")\n",
        "\n",
        "    # If working directly with text from a file (like .txt or .pdf), use that text instead.\n",
        "    # resume_text = open(\"resume.txt\").read()\n",
        "\n",
        "    # Extract details from the resume\n",
        "    resume_details = extract_resume_details(resume_text)\n",
        "\n",
        "    # Print extracted details\n",
        "    print(resume_details)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQHjPQ_UkmyE",
        "outputId": "de2e9a55-4d40-41ad-bd42-ba8cd2b21dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'Docugami', 'mobile_number': None, 'email': 'snow@email.com', 'linkedin': None, 'designation': 'WEB DEVELOPER INTERN', 'education': ['EDUCATION', '. Fast Food Worker', 'Bachelor of Science', 'Subway', '', 'Computer Science', '', 'University of Washington', '2021 - current', '', '2022 - current / Seattle, WA', '', 'e Managed 135+ transactions every day at a busy Subway outlet with', 'utmost cash handling accuracy.', '', 'Seale WA e Integrated MySQL with the POS system to update prices for 18 specials.', 'e Executed version-controlled recipe adjustments and limited-time offer', 'preparations using Git, achieving a 9% uplift in LTO sales.', 'e Collaborated with local schools to promote literacy through BookBuddy', 'Library, fostering partnerships with 4 schools.', 'e Implemented web-based tools using MongoDB to streamline inventory', 'management, shrinking manual cataloging time by 31 minutes.', 'e Organized a community book donation drive that collected 6,188 books,', 'increasing library inventory and expanding available resources by 21%.', 'e Designed a user-friendly eBook lending interface with HTML5, which', 'expanded eBook checkouts by 52%.', ''], 'skills': ['Seattle. WA Eager to sharpen my skills in database programs and markups within a team that', 'Cae pushes the boundaries of traditional document management and analysis.', 'LinkedIn', 'SKILLS . . .', 'e Leda project to redesign the layout of the store for improved workflow,', 'HTML5 shortening the time to complete orders by 6 minutes during rush hours.', 'CSS3', 'JavaScript'], 'experience': ['WORK EXPERIENCE'], 'projects': ['MongoDB PROJECTS', 'a ByteBoost Blog', 'i', '', 'Content Creator', '2023 - current', 'e Published a\"CSS3 Tips and Tricks\" weekly post that consistently ranked as', 'the most popular weekly content, boosting overall site traffic by 33%.', 'e Introduced tech challenges that encouraged the practical application of', 'blog concepts, leading to a 19% growth in active user comments.', 'e Produced a series on “JavaScript for Beginners” that attracted 283,210+', 'total views, becoming one of ByteBoost Blog’s top 7 most-read series.', 'e Adjusted the content calendar based on reader demand to ensure all', 'content was highly relevant, raising return visits by 11%.', '', 'BookBuddy Library', '', 'Participant', '2021']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_resume_details(text):\n",
        "    # Initialize extracted information\n",
        "    details = {\n",
        "        \"name\": None,\n",
        "        \"mobile_number\": None,\n",
        "        \"email\": None,\n",
        "        \"linkedin\": None,\n",
        "        \"designation\": None,\n",
        "        \"education\": [],\n",
        "        \"skills\": [],\n",
        "        \"experience\": [],\n",
        "        \"projects\": []\n",
        "    }\n",
        "\n",
        "    # Regular expressions for extracting specific information\n",
        "    mobile_pattern = re.compile(r'\\b(\\+?\\d{1,4}[-.\\s]?)?(\\d{10}|\\d{3}[-.\\s]\\d{3}[-.\\s]\\d{4})\\b')\n",
        "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')\n",
        "    linkedin_pattern = re.compile(r'(https?://)?(www\\.)?linkedin\\.com/in/[A-Za-z0-9_-]+/?')\n",
        "    designation_pattern = re.compile(r'(Intern|Engineer|Developer|Manager|Consultant|Specialist|Executive)', re.IGNORECASE)\n",
        "\n",
        "    # Section keywords\n",
        "    section_keywords = {\n",
        "        \"education\": r\"(education|bachelor|master|university|school|college|degree)\",\n",
        "        \"skills\": r\"(skills|proficiencies|technologies|competencies|technical skills)\",\n",
        "        \"experience\": r\"(work experience|employment|professional experience)\",\n",
        "        \"projects\": r\"(projects|portfolio|case study|assignments)\"\n",
        "    }\n",
        "\n",
        "    # Split text into lines\n",
        "    lines = text.split(\"\\n\")\n",
        "    current_section = None\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Extract mobile number\n",
        "        if not details[\"mobile_number\"]:\n",
        "            mobile_match = re.search(mobile_pattern, line)\n",
        "            if mobile_match:\n",
        "                details[\"mobile_number\"] = mobile_match.group()\n",
        "\n",
        "        # Extract email address\n",
        "        if not details[\"email\"]:\n",
        "            email_match = re.search(email_pattern, line)\n",
        "            if email_match:\n",
        "                details[\"email\"] = email_match.group()\n",
        "\n",
        "        # Extract LinkedIn link\n",
        "        if not details[\"linkedin\"]:\n",
        "            linkedin_match = re.search(linkedin_pattern, line)\n",
        "            if linkedin_match:\n",
        "                details[\"linkedin\"] = linkedin_match.group()\n",
        "\n",
        "        # Extract name (basic heuristic: first line, proper case)\n",
        "        if not details[\"name\"] and re.match(r'^[A-Z][a-z]+\\s[A-Z][a-z]+', line):\n",
        "            details[\"name\"] = line\n",
        "\n",
        "        # Extract designation based on job title keywords\n",
        "        if not details[\"designation\"]:\n",
        "            designation_match = re.search(designation_pattern, line)\n",
        "            if designation_match:\n",
        "                details[\"designation\"] = designation_match.group()\n",
        "\n",
        "        # Identify sections using strong keywords\n",
        "        for section, pattern in section_keywords.items():\n",
        "            if re.search(pattern, line.lower()):\n",
        "                current_section = section\n",
        "                break\n",
        "\n",
        "        # Append lines to the respective section\n",
        "        if current_section:\n",
        "            if current_section == \"education\":\n",
        "                details[\"education\"].append(line)\n",
        "            elif current_section == \"skills\":\n",
        "                details[\"skills\"].append(line)\n",
        "            elif current_section == \"experience\":\n",
        "                details[\"experience\"].append(line)\n",
        "            elif current_section == \"projects\":\n",
        "                details[\"projects\"].append(line)\n",
        "\n",
        "    return details\n",
        "\n",
        "# Sample test with the text extracted from the image\n",
        "resume_text = extract_text_from_image(\"/content/drive/MyDrive/hindi_books_RnD/data/images/020_0.png\")\n",
        "\n",
        "# Run the function to extract details\n",
        "extracted_details = extract_resume_details(resume_text)\n",
        "print(extracted_details)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dYZZ3RalfH7",
        "outputId": "54e9faf3-6dbb-495a-d426-1776da617219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'Computer Science', 'mobile_number': None, 'email': 'snow@email.com', 'linkedin': None, 'designation': 'DEVELOPER', 'education': ['EDUCATION', '. Fast Food Worker', 'Bachelor of Science', 'Subway', '', 'Computer Science', '', 'University of Washington', '2021 - current', '', '2022 - current / Seattle, WA', '', 'e Managed 135+ transactions every day at a busy Subway outlet with', 'utmost cash handling accuracy.', '', 'Seale WA e Integrated MySQL with the POS system to update prices for 18 specials.', 'e Executed version-controlled recipe adjustments and limited-time offer', 'preparations using Git, achieving a 9% uplift in LTO sales.', 'e Collaborated with local schools to promote literacy through BookBuddy', 'Library, fostering partnerships with 4 schools.', 'e Implemented web-based tools using MongoDB to streamline inventory', 'management, shrinking manual cataloging time by 31 minutes.', 'e Organized a community book donation drive that collected 6,188 books,', 'increasing library inventory and expanding available resources by 21%.', 'e Designed a user-friendly eBook lending interface with HTML5, which', 'expanded eBook checkouts by 52%.', ''], 'skills': ['Seattle. WA Eager to sharpen my skills in database programs and markups within a team that', 'Cae pushes the boundaries of traditional document management and analysis.', 'LinkedIn', 'SKILLS . . .', 'e Leda project to redesign the layout of the store for improved workflow,', 'HTML5 shortening the time to complete orders by 6 minutes during rush hours.', 'CSS3', 'JavaScript'], 'experience': ['WORK EXPERIENCE'], 'projects': ['MongoDB PROJECTS', 'a ByteBoost Blog', 'i', '', 'Content Creator', '2023 - current', 'e Published a\"CSS3 Tips and Tricks\" weekly post that consistently ranked as', 'the most popular weekly content, boosting overall site traffic by 33%.', 'e Introduced tech challenges that encouraged the practical application of', 'blog concepts, leading to a 19% growth in active user comments.', 'e Produced a series on “JavaScript for Beginners” that attracted 283,210+', 'total views, becoming one of ByteBoost Blog’s top 7 most-read series.', 'e Adjusted the content calendar based on reader demand to ensure all', 'content was highly relevant, raising return visits by 11%.', '', 'BookBuddy Library', '', 'Participant', '2021']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "import re\n",
        "\n",
        "# Set the path to the Tesseract executable if needed\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
        "\n",
        "# Step 1: Read text from an image using pytesseract\n",
        "def extract_text_from_image(image_path):\n",
        "    # Open an image file\n",
        "    img = Image.open(image_path)\n",
        "    # Use pytesseract to do OCR on the image\n",
        "    text = pytesseract.image_to_string(img)\n",
        "    return text\n",
        "\n",
        "# Step 2: Bifurcate text sections based on common headers like \"Education\", \"Skills\", etc.\n",
        "def bifurcate_text_sections(text):\n",
        "    sections = {\n",
        "        \"Education\": \"\",\n",
        "        \"Skills\": \"\",\n",
        "        \"Work Experience\": \"\",\n",
        "        \"Projects\": \"\",\n",
        "        \"Other\": \"\"  # For uncategorized content\n",
        "    }\n",
        "\n",
        "    # Define regex patterns for section headers\n",
        "    patterns = {\n",
        "        \"Education\": r\"(education|degree|university|bachelor|master|college|school)\",\n",
        "        \"Skills\": r\"(skills|technologies|proficiencies|competencies)\",\n",
        "        \"Work Experience\": r\"(work experience|professional experience|employment|career|job)\",\n",
        "        \"Projects\": r\"(projects|portfolio|case studies|assignments)\"\n",
        "    }\n",
        "\n",
        "    current_section = \"Other\"  # Track current section\n",
        "\n",
        "    # Split text into lines\n",
        "    lines = text.split(\"\\n\")\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue  # Skip empty lines\n",
        "\n",
        "        # Check for section headers in the current line\n",
        "        for section, pattern in patterns.items():\n",
        "            if re.search(pattern, line.lower()):\n",
        "                current_section = section\n",
        "                break\n",
        "        else:\n",
        "            # Assign content to the current section\n",
        "            sections[current_section] += line + \"\\n\"\n",
        "\n",
        "    return sections\n",
        "\n",
        "# Step 3: Extract Education details from the Education section\n",
        "def extract_education_details(education_section):\n",
        "    education_list = []\n",
        "\n",
        "    # Split the section into lines and look for common education keywords\n",
        "    for line in education_section.split(\"\\n\"):\n",
        "        if \"university\" in line.lower() or \"bachelor\" in line.lower() or \"master\" in line.lower():\n",
        "            education_list.append(line.strip())\n",
        "\n",
        "    return education_list\n",
        "\n",
        "# Step 4: Extract Skills details from the Skills section\n",
        "def extract_skills_details(skills_section):\n",
        "    skills = skills_section.replace('\\n', ', ')  # Skills are often listed line by line\n",
        "    return skills.strip(', ')\n",
        "\n",
        "# Step 5: Extract Work Experience details from the Work Experience section\n",
        "def extract_work_experience_details(work_experience_section):\n",
        "    experience_list = []\n",
        "\n",
        "    # Split the section into lines and filter based on experience markers\n",
        "    for line in work_experience_section.split(\"\\n\"):\n",
        "        if re.search(r\"\\d{4}\", line):  # Look for years to identify job timelines\n",
        "            experience_list.append(line.strip())\n",
        "\n",
        "    return experience_list\n",
        "\n",
        "# Main function to process the resume\n",
        "def process_resume(image_path):\n",
        "    # Step 1: Extract text from the image\n",
        "    extracted_text = extract_text_from_image(image_path)\n",
        "\n",
        "    # Step 2: Bifurcate the text into sections\n",
        "    sections = bifurcate_text_sections(extracted_text)\n",
        "\n",
        "    # Step 3: Extract information from each section\n",
        "    education = extract_education_details(sections[\"Education\"])\n",
        "    skills = extract_skills_details(sections[\"Skills\"])\n",
        "    work_experience = extract_work_experience_details(sections[\"Work Experience\"])\n",
        "\n",
        "    # Print the extracted details\n",
        "    print(\"===== Education =====\")\n",
        "    print(\"\\n\".join(education) if education else \"No Education Details Found\")\n",
        "\n",
        "    print(\"\\n===== Skills =====\")\n",
        "    print(skills if skills else \"No Skills Details Found\")\n",
        "\n",
        "    print(\"\\n===== Work Experience =====\")\n",
        "    print(\"\\n\".join(work_experience) if work_experience else \"No Work Experience Details Found\")\n",
        "\n",
        "# Provide the path to your image here\n",
        "image_path = '/content/drive/MyDrive/hindi_books_RnD/data/images/020_0.png'\n",
        "\n",
        "# Process the resume image\n",
        "process_resume(image_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scDrlZ8Xnxmf",
        "outputId": "48deaec1-7d6c-486d-faea-501a5ec23534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Education =====\n",
            "No Education Details Found\n",
            "\n",
            "===== Skills =====\n",
            "Se pushes the boundaries of traditional document management and analysis., LinkedIn, e Leda project to redesign the layout of the store for improved workflow,, HTML5 shortening the time to complete orders by 6 minutes during rush hours., CSS3, JavaScript\n",
            "\n",
            "===== Work Experience =====\n",
            "(123) 456-7890 innovative approach to document engineering as a web development intern.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import re\n",
        "\n",
        "# Preprocessing to improve Tesseract performance\n",
        "def preprocess_image(image_path):\n",
        "    # Load the image in grayscale\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Resize the image if necessary (optional)\n",
        "    image = cv2.resize(image, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Apply thresholding to binarize the image\n",
        "    _, image = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Text extraction from the image\n",
        "def extract_text_from_image(image):\n",
        "    custom_config = r'--oem 3 --psm 6'  # Use OCR Engine mode 3 and Page Segmentation mode 6 for more accuracy\n",
        "    text = pytesseract.image_to_string(image, config=custom_config)\n",
        "    return text\n",
        "\n",
        "# Helper function to clean up and split extracted text\n",
        "def clean_and_split_text(text):\n",
        "    text = text.strip()\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = [line.strip() for line in lines if line.strip()]\n",
        "    return cleaned_lines\n",
        "\n",
        "# Extract sections like Education, Skills, Work Experience, etc.\n",
        "def segment_text_by_sections(text_lines):\n",
        "    sections = {\n",
        "        \"education\": [],\n",
        "        \"skills\": [],\n",
        "        \"experience\": [],\n",
        "        \"projects\": []\n",
        "    }\n",
        "\n",
        "    current_section = None\n",
        "    section_keywords = {\n",
        "        \"education\": r\"(education|bachelor|master|university|school|college|degree)\",\n",
        "        \"skills\": r\"(skills|proficiencies|technologies|competencies|technical skills)\",\n",
        "        \"experience\": r\"(experience|employment|work history|internship|professional experience)\",\n",
        "        \"projects\": r\"(projects|portfolio|case study|assignments)\"\n",
        "    }\n",
        "\n",
        "    for line in text_lines:\n",
        "        for section, pattern in section_keywords.items():\n",
        "            if re.search(pattern, line.lower()):\n",
        "                current_section = section\n",
        "                break\n",
        "\n",
        "        if current_section:\n",
        "            sections[current_section].append(line)\n",
        "\n",
        "    return sections\n",
        "\n",
        "# Extract specific fields from each section\n",
        "def extract_resume_details(sections):\n",
        "    details = {\n",
        "        \"name\": None,\n",
        "        \"mobile_number\": None,\n",
        "        \"email\": None,\n",
        "        \"linkedin\": None,\n",
        "        \"designation\": None,\n",
        "        \"education\": None,\n",
        "        \"skills\": None,\n",
        "        \"experience\": None,\n",
        "        \"projects\": None\n",
        "    }\n",
        "\n",
        "    # Define patterns for common fields\n",
        "    mobile_pattern = re.compile(r'\\b(\\+?\\d{1,4}[-.\\s]?)?(\\d{10}|\\d{3}[-.\\s]\\d{3}[-.\\s]\\d{4})\\b')\n",
        "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')\n",
        "    linkedin_pattern = re.compile(r'\\b(https?://)?(www\\.)?linkedin\\.com/in/[A-Za-z0-9_-]+/?\\b')\n",
        "\n",
        "    # Extract from the raw sections\n",
        "    for section, content in sections.items():\n",
        "        if section == \"education\" and content:\n",
        "            details[\"education\"] = ' '.join(content)  # Combine education lines into a string\n",
        "        elif section == \"skills\" and content:\n",
        "            details[\"skills\"] = ' '.join(content)  # Combine skills lines into a string\n",
        "        elif section == \"experience\" and content:\n",
        "            details[\"experience\"] = ' '.join(content)  # Combine experience lines into a string\n",
        "        elif section == \"projects\" and content:\n",
        "            details[\"projects\"] = ' '.join(content)  # Combine project lines into a string\n",
        "\n",
        "        # Extract fields common to any section\n",
        "        for line in content:\n",
        "            if not details[\"mobile_number\"]:\n",
        "                mobile_match = re.search(mobile_pattern, line)\n",
        "                if mobile_match:\n",
        "                    details[\"mobile_number\"] = mobile_match.group()\n",
        "\n",
        "            if not details[\"email\"]:\n",
        "                email_match = re.search(email_pattern, line)\n",
        "                if email_match:\n",
        "                    details[\"email\"] = email_match.group()\n",
        "\n",
        "            if not details[\"linkedin\"]:\n",
        "                linkedin_match = re.search(linkedin_pattern, line)\n",
        "                if linkedin_match:\n",
        "                    details[\"linkedin\"] = linkedin_match.group()\n",
        "\n",
        "    return details\n",
        "\n",
        "# Main function to run the whole process\n",
        "def parse_resume_image(image_path):\n",
        "    # Step 1: Preprocess the image\n",
        "    processed_image = preprocess_image(image_path)\n",
        "\n",
        "    # Step 2: Extract text from the image\n",
        "    extracted_text = extract_text_from_image(processed_image)\n",
        "\n",
        "    # Step 3: Clean and split the text into lines\n",
        "    text_lines = clean_and_split_text(extracted_text)\n",
        "\n",
        "    # Step 4: Segment the text by sections\n",
        "    sections = segment_text_by_sections(text_lines)\n",
        "\n",
        "    # Step 5: Extract details from each section\n",
        "    resume_details = extract_resume_details(sections)\n",
        "\n",
        "    return resume_details\n",
        "\n",
        "# Example usage\n",
        "resume_image_path = '/content/drive/MyDrive/hindi_books_RnD/data/images/020_0.png'\n",
        "parsed_resume = parse_resume_image(resume_image_path)\n",
        "print(parsed_resume)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Bs6hd6wpB41",
        "outputId": "61be0dfc-6f20-4c31-b3d3-577a4dd4a8ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': None, 'mobile_number': None, 'email': None, 'linkedin': None, 'designation': None, 'education': 'EDUCATION , Fast Food Worker Bachelor of Science Sub Computer Science ubway . . . 2022 - current / Seattle, WA University of Washington ; . 2021 - current e Managed 135+ transactions every day at a busy Subway outlet with utmost cash handling accuracy. Seattle, WA e Integrated MySQL with the POS system to update prices for 18 specials. e Executed version-controlled recipe adjustments and limited-time offer e Collaborated with local schools to promote literacy through BookBuddy Library, fostering partnerships with 4 schools. e Implemented web-based tools using MongoDB to streamline inventory management, shrinking manual cataloging time by 31 minutes. e Organized a community book donation drive that collected 6,188 books, increasing library inventory and expanding available resources by 21%. e Designed a user-friendly eBook lending interface with HTML5, which expanded eBook checkouts by 52%.', 'skills': 'Seattle. WA Eager to sharpen my skills in database programs and markups within a team that cattle, pushes the boundaries of traditional document management and analysis. LinkedIn SKILLS preparations using Git, achieving a 9% uplift in LTO sales. e Leda project to redesign the layout of the store for improved workflow, HTML5 shortening the time to complete orders by 6 minutes during rush hours. CSS3 JavaScript i ByteBoost Blog i Content Creator 2023 - current e Published a \"CSS3 Tips and Tricks\" weekly post that consistently ranked as the most popular weekly content, boosting overall site traffic by 33%. ® Introduced tech challenges that encouraged the practical application of blog concepts, leading to a 19% growth in active user comments. e Produced a series on “JavaScript for Beginners” that attracted 283,210+ total views, becoming one of ByteBoost Blog’s top 7 most-read series. e Adjusted the content calendar based on reader demand to ensure all content was highly relevant, raising return visits by 11%. BookBuddy Library Participant 2021', 'experience': 'WORK EXPERIENCE', 'projects': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "\n",
        "# Load the image\n",
        "image_path = '/content/drive/MyDrive/hindi_books_RnD/data/images/020_0.png'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Pre-process the image for better OCR results\n",
        "# Convert to grayscale\n",
        "image = image.convert('L')\n",
        "\n",
        "# Enhance the contrast\n",
        "enhancer = ImageEnhance.Contrast(image)\n",
        "image = enhancer.enhance(2)\n",
        "\n",
        "# Apply some sharpening\n",
        "image = image.filter(ImageFilter.SHARPEN)\n",
        "\n",
        "# Resize the image if necessary to improve OCR accuracy\n",
        "width, height = image.size\n",
        "image = image.resize((width * 2, height * 2))\n",
        "\n",
        "# Perform OCR on the processed image\n",
        "text = pytesseract.image_to_string(image)\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-YV3eflsFqS",
        "outputId": "0ef17b03-e634-4f2f-d13d-c515ff401d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LEOCADIA SNOW\n",
            "\n",
            "WEB DEVELOPER INTERN\n",
            "\n",
            " \n",
            "\n",
            "CONTACT\n",
            "\n",
            "I.snow@email.com |\n",
            "(123) 456-7890\n",
            "Seattle, WA\n",
            "LinkedIn F\n",
            "\n",
            "EDUCATION\n",
            "\n",
            "Bachelor of Science\n",
            "Computer Science\n",
            "University of Washington\n",
            "2021 - current\n",
            "\n",
            "Seattle, WA\n",
            "\n",
            "SKILLS\n",
            "\n",
            "HTML5\n",
            "CSS3\n",
            "JavaScript\n",
            "MongoDB\n",
            "MySQL\n",
            "Git\n",
            "\n",
            "CAREER OBJECTIVE\n",
            "\n",
            "As asenior computer science student, | aim to contribute to Docugami’s\n",
            "innovative approach to document engineering as a web development intern.\n",
            "Eager to sharpen my skills in database programs and markups within a team that\n",
            "pushes the boundaries of traditional document management and analysis.\n",
            "\n",
            "WORK EXPERIENCE\n",
            "\n",
            "Fast Food Worker\n",
            "Subway\n",
            "2022 -current / Seattle, WA\n",
            "\n",
            "e Managed 135+ transactions every day at a busy Subway outlet with\n",
            "utmost cash handling accuracy.\n",
            "\n",
            "e Integrated MySQL with the POS system to update prices for 18 specials.\n",
            "\n",
            "e Executed version-controlled recipe adjustments and limited-time offer\n",
            "preparations using Git, achieving a 9% uplift in LTO sales.\n",
            "\n",
            "e Leda project to redesign the layout of the store for improved workflow,\n",
            "shortening the time to complete orders by 6 minutes during rush hours.\n",
            "\n",
            "PROJECTS\n",
            "ByteBoost Blog\n",
            "\n",
            "Content Creator\n",
            "2023 - current\n",
            "e Published a\"CSS3 Tips and Tricks\" weekly post that consistently ranked as\n",
            "the most popular weekly content, boosting overall site traffic by 33%.\n",
            "e Introduced tech challenges that encouraged the practical application of\n",
            "blog concepts, leading to a 19% growth in active user comments.\n",
            "e Produced a series on “JavaScript for Beginners” that attracted 283,210+\n",
            "total views, becoming one of ByteBoost Blog’s top 7 most-read series.\n",
            "e Adjusted the content calendar based on reader demand to ensure all\n",
            "content was highly relevant, raising return visits by 11%.\n",
            "\n",
            "BookBuddy Library\n",
            "\n",
            "Participant\n",
            "2021\n",
            "\n",
            "e Collaborated with local schools to promote literacy through BookBuddy\n",
            "Library, fostering partnerships with 4 schools.\n",
            "\n",
            "e Implemented web-based tools using MongoDB to streamline inventory\n",
            "management, shrinking manual cataloging time by 31 minutes.\n",
            "\n",
            "e Organized a community book donation drive that collected 6,188 books,\n",
            "increasing library inventory and expanding available resources by 21%.\n",
            "\n",
            "e Designed a user-friendly eBook lending interface with HTML5, which\n",
            "expanded eBook checkouts by 52%.\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Fybqz45y71H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "# Improved extraction function\n",
        "def extract_resume_details(text):\n",
        "    # Initialize extracted information\n",
        "    details = {\n",
        "        \"Name\": None,\n",
        "        \"E-mail\": None,\n",
        "        \"Degree\": None,\n",
        "        \"Current Company\": None,\n",
        "        \"City\": None,\n",
        "        \"Skills\": None,\n",
        "        \"Experience in month\": None,\n",
        "        \"Linkedin\": None,\n",
        "        \"Year of passing\": None\n",
        "    }\n",
        "\n",
        "    # Regular expressions for extracting specific information\n",
        "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
        "    linkedin_pattern = re.compile(r'(https?:\\/\\/)?(www\\.)?linkedin\\.com\\/[a-zA-Z0-9\\/\\-]+')\n",
        "    degree_pattern = re.compile(r'(degree)')\n",
        "    experience_pattern = re.compile(r'(\\d+)\\s?(years?|months?)\\s?(experience)?', re.IGNORECASE)\n",
        "    year_pattern = re.compile(r'\\b(19|20)\\d{2}\\b')  # Years in the range 1900-2099\n",
        "    city_pattern = re.compile(r'(Location|City|Based in)\\s*[:\\-]?\\s*([A-Za-z\\s]+)')\n",
        "    company_pattern = re.compile(r'(Company|Work at|Employer)\\s*[:\\-]?\\s*([A-Za-z\\s]+)')\n",
        "    skills_pattern = re.compile(r'(skills|technologies|technical skills)')\n",
        "\n",
        "    # Split text into lines\n",
        "    lines = text.split(\"\\n\")\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Extract email address\n",
        "        if not details[\"E-mail\"]:\n",
        "            email_match = re.search(email_pattern, line)\n",
        "            if email_match:\n",
        "                details[\"E-mail\"] = email_match.group()\n",
        "\n",
        "        # Extract LinkedIn link\n",
        "        if not details[\"Linkedin\"]:\n",
        "            linkedin_match = re.search(linkedin_pattern, line)\n",
        "            if linkedin_match:\n",
        "                details[\"Linkedin\"] = linkedin_match.group()\n",
        "\n",
        "        # Extract Degree\n",
        "        if not details[\"Degree\"]:\n",
        "            degree_match = re.search(degree_pattern, line)\n",
        "            if degree_match:\n",
        "                details[\"Degree\"] = degree_match.group()\n",
        "\n",
        "        # Extract Experience in months or years\n",
        "        if not details[\"Experience in month\"]:\n",
        "            experience_match = re.search(experience_pattern, line)\n",
        "            if experience_match:\n",
        "                num = int(experience_match.group(1))\n",
        "                unit = experience_match.group(2).lower()\n",
        "                if 'year' in unit:\n",
        "                    details[\"Experience in month\"] = num * 12  # Convert years to months\n",
        "                else:\n",
        "                    details[\"Experience in month\"] = num  # Already in months\n",
        "\n",
        "        # Extract Year of Passing (assuming it relates to education)\n",
        "        if not details[\"Year of passing\"]:\n",
        "            year_match = re.search(year_pattern, line)\n",
        "            if year_match:\n",
        "                details[\"Year of passing\"] = year_match.group()\n",
        "\n",
        "        # Extract City\n",
        "        if not details[\"City\"]:\n",
        "            city_match = re.search(city_pattern, line)\n",
        "            if city_match:\n",
        "                details[\"City\"] = city_match.group(2).strip()\n",
        "\n",
        "        # Extract Current Company\n",
        "        if not details[\"Current Company\"]:\n",
        "            company_match = re.search(company_pattern, line)\n",
        "            if company_match:\n",
        "                details[\"Current Company\"] = company_match.group(2).strip()\n",
        "\n",
        "        # Extract Skills\n",
        "        if not details[\"Skills\"]:\n",
        "            skills_match = re.search(skills_pattern, line)\n",
        "            if skills_match:\n",
        "                details[\"Skills\"] = skills_match.group(2).strip()\n",
        "\n",
        "        # Extract Name (basic heuristic: first line, proper case)\n",
        "        if not details[\"Name\"] and re.match(r'(?<![\\w\\d])([A-Z][a-zA-Z]{1,}(?:\\s[A-Z][a-zA-Z]{1,}){0,2})(?![\\w\\d])', line):\n",
        "            details[\"Name\"] = line\n",
        "\n",
        "    return details\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ynbbdW5py7yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "from pytesseract import Output\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources (if not already done)\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# Stronger extraction by adding specific keywords and patterns\n",
        "def extract_resume_details(text):\n",
        "    # Initialize extracted information\n",
        "    details = {\n",
        "        \"name\": None,\n",
        "        \"mobile_number\": None,\n",
        "        \"email\": None,\n",
        "        \"linkedin\": None,\n",
        "        \"education\": None,\n",
        "        \"skills\": None,\n",
        "        \"experience_in_month\": None,\n",
        "        \"year_of_passing\": None\n",
        "        # \"projects\": None,\n",
        "        # \"achivements\": None,\n",
        "        # \"career-objective\": None\n",
        "    }\n",
        "\n",
        "    # Regular expressions for extracting specific information\n",
        "    mobile_pattern = re.compile(r'\\+?\\d{1,3}[-.\\s]?\\(?\\d{2,4}\\)?[-.\\s]?\\d{3,4}[-.\\s]?\\d{4,6}\\b')\n",
        "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
        "    linkedin_pattern = re.compile(r'\\b(?:https?:\\/\\/)?(?:www\\.)?linkedin\\.com\\/(?:in\\/[\\w-]+|pub\\/[\\w-]+|company\\/[\\w-]+|school\\/[\\w-]+|groups\\/[\\w-]+|events\\/[\\w-]+|[\\w-]+(?:\\/[\\w-]*)?)\\b')\n",
        "    experience_in_month = re.compile(r'(\\d+)\\s?(years?|months?)\\s?(experience)?', re.IGNORECASE)\n",
        "    year_of_passing = re.compile(r'\\b(19|20)\\d{2}\\b')\n",
        "    # Define strong keywords or patterns for different sections\n",
        "    patterns = {\n",
        "        \"education\": r\"(degree)\",\n",
        "        \"skills\": r\"(skills|technical skills)\",\n",
        "        # \"experience\": r\"(experience in month)\",\n",
        "        # \"career-objective\": r\"(career objective|objective|professional summary|career summary|summary)\",\n",
        "        # \"projects\": r\"(projects|project)\",\n",
        "        # \"achivements\": r\"(certificate|awards|achivements|certifications)\"\n",
        "    }\n",
        "\n",
        "    # Split text into lines\n",
        "    lines = text.split(\"\\n\")\n",
        "    current_section = None\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Extract mobile number\n",
        "        if not details[\"mobile_number\"]:\n",
        "            mobile_match = re.search(mobile_pattern, line)\n",
        "            if mobile_match:\n",
        "                details[\"mobile_number\"] = mobile_match.group()\n",
        "\n",
        "\n",
        "        # Extract email address\n",
        "        if not details[\"email\"]:\n",
        "            email_match = re.search(email_pattern, line)\n",
        "            if email_match:\n",
        "                details[\"email\"] = email_match.group()\n",
        "\n",
        "        # Extract LinkedIn link\n",
        "        if not details[\"linkedin\"]:\n",
        "            linkedin_match = re.search(linkedin_pattern, line)\n",
        "            if linkedin_match:\n",
        "                details[\"linkedin\"] = linkedin_match.group()\n",
        "\n",
        "\n",
        "        if not details[\"experience_in_month\"]:\n",
        "            experience_match = re.search(experience_in_month, line)\n",
        "            if experience_match:\n",
        "                num = int(experience_match.group(1))\n",
        "                unit = experience_match.group(2).lower()\n",
        "                if 'year' in unit:\n",
        "                    details[\"experience_in_month\"] = num * 12  # Convert years to months\n",
        "                else:\n",
        "                    details[\"experience_in_month\"] = num  # Already in months\n",
        "\n",
        "        # Extract Year of Passing (assuming it relates to education)\n",
        "        if not details[\"year_of_passing\"]:\n",
        "            year_match = re.search(year_of_passing, line)\n",
        "            if year_match:\n",
        "                details[\"year_of_passing\"] = year_match.group()\n",
        "\n",
        "        # Extract name (basic heuristic: first line, proper case)\n",
        "        if not details[\"name\"] and re.match(r'(?<![\\w\\d])([A-Z][a-zA-Z]{1,}(?:\\s[A-Z][a-zA-Z]{1,}){0,2})(?![\\w\\d])', line):\n",
        "            details[\"name\"] = line\n",
        "\n",
        "        # Identify sections using strong keywords\n",
        "        for section, pattern in patterns.items():\n",
        "            if re.search(pattern, line.lower()):\n",
        "                current_section = section\n",
        "                if details[current_section] is None:\n",
        "                    details[current_section] = []\n",
        "                break\n",
        "\n",
        "        # Append lines to the current section\n",
        "        if current_section and details[current_section] is not None:\n",
        "            details[current_section].append(line)\n",
        "\n",
        "    return details"
      ],
      "metadata": {
        "id": "dF0B_fmMyAFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "id": "OEbxH4GaBc0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image and perform OCR with Tesseract\n",
        "image = Image.open(\"/content/resu_new.jpg\")  # Load the uploaded image\n",
        "extracted_text = pytesseract.image_to_string(image)\n",
        "details = extract_resume_details(extracted_text)\n",
        "\n",
        "# Print the extracted details\n",
        "for key, value in details.items():\n",
        "    print(f\"{key} : {value if value else 'Not Found'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujqGFemMy7vl",
        "outputId": "065a641b-a698-46a3-d878-e33f4bd029f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name : CARLA\n",
            "mobile_number : Not Found\n",
            "email : help@enhancv.com\n",
            "linkedin : Not Found\n",
            "education : [\"Data enthusiast with a Master's degree in Data Science and 2 years of\", 'experience. Advanced knowledge in machine learning and statistical analysis.', 'Proven track record of designing and implementing data-driven solutions for', 'business improvement.', '', 'EXPERIENCE', 'Junior Data Scientist 2018 - 2019', 'IBM New York, USA', '', \"Worked in IBM's data science team focusing on implementation of machine\", '', 'learning models.', '', '« Developed and implemented a machine learning model that improved', 'efficiency by 20%', '', '« Managed large datasets using Python and SQL, streamlining data collection by', '30%', '', '¢ Collaborated with the team to designed new statistical analysis models', '', 'Assistant Data Analyst 2019 - 2020', '', 'Google California, USA', '', 'Assisted in analyzing large sets of data and providing insights to the marketing', '', 'team.', '', '¢ Assisted in pulling and analyzing large datasets, cutting operation time by 15%', '', '¢ Contributed to marketing strategy by providing data-driven insights, boosting', 'leads by 10%', '', '¢ Managed database using SQL, which ensured data integrity and availability', '', 'Business Intelligence Analyst 2020 - 2021', '', 'Microsoft Washington, USA', '', 'Analyzed data to draw business-relevant conclusions and in data visualization', '', 'techniques', '', '¢ Helped to identify business trends utilizing real data, compile analysis reports', 'that were presented to the team and the CEO', '', '¢« Implemented a new data analysis tool which improved the data processing', 'speed by 25%', '', '¢ Worked on the creation of a new BI dashboard that increased data accuracy', 'by 15%', '', 'EDUCATION', '', 'BSc in Computer Science 2010 - 2014', '', 'University of Southern California California, USA', '', 'MSc in Data Science 2015 - 2018', '', 'New York University New York, USA', '', 'LANGUAGES', '', 'English Native eeceee Spanish Advanced eee', '', 'www.enhancv.com', '']\n",
            "skills : ['SKILLS', '', 'Python: SQL - Machine Learning -', 'Tableau: R_ - Data Visualization -', '', 'Statistical Analysis - Data Mining', '', 'CERTIFICATION', '', 'Python for Data Science', '', 'Certification course by the', 'University of Michigan, offered', 'through Coursera', '', 'Applied Data Science with', 'Python', '', 'Certification course by the', 'University of Michigan, offered', 'through Coursera', '', 'PASSIONS', '', '® Artificial Intelligence', '', 'Always fascinated by the', 'possibilities of Al and', 'machine learning', '', 'Mountain Biking', '', 'Regularly participate in local', 'biking events and adventures', '', 'Powered by (CX?) Enhancu', '', '', '', 'Data Enthusiast | Machine Learning | Python', 'Developer', '', 'help@enhancv.com linkedin.com undefined', '', 'SUMMARY', '']\n",
            "experience_in_month : 24\n",
            "year_of_passing : 2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1__5vVfbBFH_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}